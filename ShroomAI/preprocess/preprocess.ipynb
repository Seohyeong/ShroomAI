{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/seohyeong/Projects/ShroomAI/ShroomAI/preprocess'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import hashlib\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import wikipedia\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing Occurrence.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimedia_txt_path = '../dataset/raw_inat/multimedia.txt'\n",
    "occurrence_txt_path = '../dataset/raw_inat/occurrence.txt'\n",
    "\n",
    "train_df_save_path = '../dataset/train_df.txt'\n",
    "test_df_save_path = '../dataset/test_df.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_df = pd.read_csv(occurrence_txt_path, sep = '\\t', dtype=str)\n",
    "occ_df = occ_df[['gbifID', 'catalogNumber', \n",
    "                 'year', 'month', \n",
    "                 'continent', 'countryCode', 'stateProvince', 'decimalLatitude', 'decimalLongitude', \n",
    "                 'taxonID', 'phylum', 'class', 'order', 'family', 'genus', 'taxonKey', 'speciesKey', 'species']]\n",
    "occ_df = occ_df[occ_df['phylum'].isin(['Basidiomycota', 'Ascomycota'])]\n",
    "occ_df = occ_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_train_df = occ_df[~occ_df['year'].isin(['2024'])]\n",
    "occ_test_df = occ_df[occ_df['year'] == '2024']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Selecting Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_stats = Counter(occ_train_df['species']).most_common()\n",
    "species_keep = [species for species, _ in species_stats][:1000]\n",
    "occ_train_df = occ_train_df[occ_train_df['species'].isin(species_keep)]\n",
    "occ_test_df = occ_test_df[occ_test_df['species'].isin(species_keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing Multimedia.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_df = pd.read_csv(multimedia_txt_path, sep='\\t', dtype=str)\n",
    "mm_df = mm_df[['gbifID', 'identifier']]\n",
    "mm_df = mm_df.dropna()\n",
    "mm_df = mm_df.drop_duplicates(subset=['identifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_train_df = pd.merge(occ_train_df, mm_df, on='gbifID', how='inner')\n",
    "occ_test_df = pd.merge(occ_test_df, mm_df, on='gbifID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value(value):\n",
    "    return hashlib.md5(str(value).encode()).hexdigest()\n",
    "occ_train_df['uniqueID'] = occ_train_df['identifier'].apply(hash_value)\n",
    "occ_test_df['uniqueID'] = occ_test_df['identifier'].apply(hash_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_train_df.to_csv(train_df_save_path, sep='\\t', index=False)\n",
    "occ_test_df.to_csv(test_df_save_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scraping Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Create Meta File with Additional Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - taxonID: inat species id (https://www.inaturalist.org/taxa/{taxonID})\n",
    "# - speciesKey: gbif species id (https://www.gbif.org/species/{speciesKey}\n",
    "\n",
    "test_df_path = '../dataset/test_df.txt'\n",
    "meta_json_path = '../dataset/meta.json'\n",
    "\n",
    "df = pd.read_csv(test_df_path, sep='\\t')\n",
    "\n",
    "species_info = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['species'] not in species_info.keys():\n",
    "        info = {row['species']: {\n",
    "            # identifier\n",
    "            'gbifOccID': row['gbifID'],\n",
    "            'gbifSpeciesID': row['speciesKey'],\n",
    "            'inatSpeciesID': row['taxonID'],\n",
    "            # verbatim\n",
    "            'phylum': row['phylum'],\n",
    "            'class': row['class'],\n",
    "            'order': row['order'],\n",
    "            'family': row['family'],\n",
    "            'genus': row['genus'],\n",
    "        }\n",
    "                }\n",
    "        species_info.update(info)\n",
    "        \n",
    "with open(meta_json_path, 'w') as outfile: \n",
    "    json.dump(species_info, outfile, indent=4)\n",
    "\n",
    "print(len(species_info)) # shoule match the number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Scrape Nickname & Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(meta_json_path, 'r') as file:\n",
    "    species_info = json.load(file)\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "for species, info in tqdm(species_info.items(), total=len(species_info)):\n",
    "    inat_species_url = 'https://www.inaturalist.org/taxa/{}'.format(info['inatSpeciesID'])\n",
    "    response = requests.get(inat_species_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # get common name\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        title_text = title_tag.get_text()\n",
    "        common_name = title_text.split(' (')[0]\n",
    "        species_info[species]['commonName'] = common_name\n",
    "    else:\n",
    "        species_info[species]['commonName'] = ''\n",
    "        print('common name not found.')\n",
    "    \n",
    "    # get wiki desc\n",
    "    try:\n",
    "        desc = wikipedia.summary(species)\n",
    "        species_info[species]['desc'] = desc\n",
    "    except:\n",
    "        species_info[species]['desc'] = ''\n",
    "        print(\"Wikipedia desc not found.\")\n",
    "        not_found += 1\n",
    "            \n",
    "print(\"wiki desc not_found: \", not_found)\n",
    "with open(meta_json_path, 'w') as outfile: \n",
    "    json.dump(species_info, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Change meta.json Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(meta_json_path, 'r') as file:\n",
    "    species_info = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_species_info = []\n",
    "\n",
    "for species, info in species_info.items():\n",
    "    item = {'species': species}\n",
    "    item.update(info)\n",
    "    flatten_species_info.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(meta_json_path, 'w') as outfile: \n",
    "    json.dump(flatten_species_info, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract an image for each species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dataset_path = '/Users/seohyeong/Projects/ShroomAI/ShroomAI/dataset/inat_300/train'\n",
    "sample_images_path = '/Users/seohyeong/Projects/ShroomAI/ShroomAI/dataset/sample_images'\n",
    "\n",
    "# os.mkdir(sample_images_path)\n",
    "\n",
    "for species in os.listdir(dataset_path):\n",
    "    species_path = os.path.join(dataset_path, species)\n",
    "    image_name = os.listdir(species_path)[0]\n",
    "    example_img_path = os.path.join(species_path, image_name)\n",
    "    new_example_img_path = os.path.join(sample_images_path, species + \".jpg\")\n",
    "    shutil.copy(example_img_path, new_example_img_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shroomai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
